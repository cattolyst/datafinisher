""" Generate dynamic data extraction SQL for DataBuilder output files
---------------------------------------------------------------------
    
 Usage:
   makesql sqltemplate.sql dbname.db
"""

import sqlite3 as sq,argparse,re,csv,time

parser = argparse.ArgumentParser()
parser.add_argument("dbfile",help="SQLite file generated by DataBuilder")
parser.add_argument("-c","--cleanup",help="Restore dbfile to its vanilla, data-builder state",action="store_true")
parser.add_argument("-v","--csvfile",help="File to write output to, in addition to the tables that will get created in the dbfile. By default this is whatever was the name of the dbfile with '.csv' substituted for '.db'",default='OUTFILE')
parser.add_argument("-s","--style",help="What style to output the file, currently there are two-- concat which concatenates the code variables and simple which represents code variables as Yes/No, with the nulls represented by No. The default is concat.",default="concat",choices=['concat','simple'])
parser.add_argument("-d","--datecompress",help="Round all dates to the nearest X days, default is 1",default=1)
args = parser.parse_args()

# location of data dictionary sql file
ddsql = "sql/dd.sql"

#pull in all the SQLite script as variables from the config file
import ConfigParser
cfg = ConfigParser.RawConfigParser()
cfg.read('sqldump.cfg')
par=dict(cfg.items("Settings"))

# okay, this actually works
class diaggregate:
  def __init__(self):
    self.cons = {}
    self.oocm = {}; self.ooc = []
  def step(self,con,mod):
    if con not in self.cons.keys():
      self.cons[con] = [mod]
    else:
      if mod not in self.cons[con]:
	self.cons[con].append(mod)
  def finalize(self):
    for ii in self.cons:
      iimods = [jj for jj in self.cons[ii] if jj not in  ['@',None,'']]
      if len(iimods) == 0:
	self.ooc.append('"'+ii+'"')
      else:
	self.oocm[ii] = iimods
    #oo += ['"'+ii+'":["'+'","'.join(self.cons[ii])+'"]' for ii in self.cons]
    #oo = ",".join(oo)
    return ",".join(self.ooc+['"'+ii+'":["'+'","'.join(self.oocm[ii])+'"]' for ii in self.oocm])
  
class infoaggregate:
  # generically jam together the ancillary fields to see if there is anything noteworthy anywhere in there
  # note that normally you would use NULL or '' for some of these params (to bypass them), doing the aggregation 
  # only on the ones you don't expect to see
  def __init__(self):
    self.cons = {}
  def step(self,con,mod,ins,vtp,tvc,nvn,vfl,qty,unt,loc,cnf):
    self.ofvars = {'cc':str(con),'mc':str(mod),'ix':str(ins),'vt':str(vtp),'tc':str(tvc),'vf':str(vfl),'qt':str(qty),'un':str(unt),'lc':str(loc),'cf':str(cnf)}
    # go through each possible arg, check if it's NULL/@/''
    # if not, add to self.cons
    if nvn not in ['@','None',None,'']:
      if 'nv' not in self.cons.keys():
	self.cons['nv'] = 1
      else:
	self.cons['nv'] += 1
    for ii in self.ofvars:
      if self.ofvars[ii] not in ['@','None',None,'']:
	if ii not in self.cons.keys():
	  self.cons[ii] = [self.ofvars[ii]]
	elif self.ofvars[ii] not in self.cons[ii]:
	  self.cons[ii] += [self.ofvars[ii]]
  def finalize(self):
    # oh... python's dictionary format looks just like JSON, and you can convert it to a string
    # the replace calls are just to make it a little more compact
    if 'nv' in self.cons.keys():
      if self.cons['nv']==1:
	del self.cons['nv']
      else:
	self.cons['nv'] = str(self.cons['nv'])
    if 'ix' in self.cons.keys():
      if self.cons['ix'] == ['1']:
	del self.cons['ix']
    return (str(self.cons)[1:-1]).replace("', '","','").replace(": ",":")

class debugaggregate:
  # this is the kitchen-sink aggregator-- doesn't really condense the data, rather the purpose is to preserve everything there is to be
  # known about each OBSERVATION_FACT entry while still complying with the one-row-per-patient-date requirement
  def __init__(self):
    self.entries = []
  def step(self,cc,mc,ix,vt,tc,nv,vf,qt,un,lc,cf):
    self.entries.append(",".join(['"'+ii+'":"'+str(vars()[ii])+'"' for ii in ['cc','mc','ix','vt','tc','nv','vf','qt','un','lc','cf'] if vars()[ii] not in ['@',None,'','None']]))
  def finalize(self):
    return "{"+"},{".join(self.entries)+"}"

# this is to register a SQLite function for pulling out matching substrings (if found)
# and otherwise returning the original string. Useful for extracting ICD9, CPT, and LOINC codes
# from concept paths where they are embedded. For ICD9 the magic pattern is:
# '.*\\\\([VE0-9]{3}\.{0,1}[0-9]{0,2})\\\\.*'
def ifgrp(pattern,txt):
    rs = re.search(re.compile(pattern),txt)
    
    if rs == None:
      return txt 
    else:
      return rs.group(1)

def cleanup(cnx):
    t_drop = ['cdid','codefacts','codemodfacts','diagfacts','loincfacts','data_dictionary',
	      'fulloutput','oneperdayfacts','scaffold','unkfacts','unktemp','dfvars']
    v_drop = ['obs_all','obs_diag_active','obs_diag_inactive','obs_labs','obs_noins']
    print "Dropping views"
    for ii in v_drop:
      cnx.execute("drop view if exists "+ii)
    print "Dropping tables"
    for ii in t_drop:
      cnx.execute("drop table if exists "+ii)

def tprint(str,tt):
    print(str+":"+" "*(60-len(str))+"%9.4f" % round((time.time() - tt),4))
      
# The rdt and rdst functions aren't exactly user-defined SQLite functions...
# They are python function that emit a string to concatenate into a larger SQL query
# and send back to SQL... because SQLite has a native julianday() function that's super
# easy to use. So, think of rdt and rdst as pseudo-UDFs
def rdt(datecol,factor):
    if factor == 1:
      return 'date('+datecol+')'
    else:
      return 'date(round(julianday('+datecol+')/'+str(factor)+')*'+str(factor)+')'
    
def rdst(factor):
    return rdt('start_date',factor)

def dfctday(**kwargs):                                          
  if kwargs is not None:
    oo = "replace(group_concat(distinct '{'||"
    for key,val in kwargs.iteritems():
      oo += """coalesce('{0}:"'||{1}||'",','')||""".format(key,val)
    oo += "'}'),',}','}')"                                             
    return oo
  
def dfctcode(**kwargs):
   if kwargs is not None:
     oo = ""
     for key,val in kwargs.iteritems():
       oo += """coalesce('{0}:['||group_concat(distinct '"'||{1}||'"')||'],','')||""".format(key,val)
     return oo[:-2].replace('],',']')

def shortenwords(words,limit):
	""" Initialize the data, lengths, and indexes"""
	#get rid of the numeric codes
	words = re.sub('[0-9]','',words)
	wrds = words.split(); lens = map(len,wrds); idxs=range(len(lens))
	if limit >= len(words):
	  return(words)
	""" sort the indexes and lengths"""
	idxs.sort(key=lambda xx: lens[xx]); lens.sort()
	""" initialize the threshold and the vector of 'most important' words"""
	sumidx=0; keep=[]
	# turned out that checking the lengths of the lens and idxs is what it takes to avoid crashes
	while sumidx < limit and len(lens) > 0 and len(idxs) > 0:
		sumidx += lens.pop()
		keep.append(idxs.pop())
	keep.sort()
	shortened = [wrds[ii] for ii in keep]
	return " ".join(shortened)

def dropletters(intext):
	# This function shortens words by squeezing out vowels, most non-alphas, and repeating letters
	# the first regexp replaces multiple ocurrences of the same letter with one ocurrence of that letter
	# the \B matches a word boundary... so we only remove vowels from inside words, not leading lettters
	return re.sub(r"([a-z_ ])\1",r"\1",re.sub("\B[aeiouyAEIOUY]+","",re.sub("[^a-zA-Z _]"," ", intext)))


def main(cnx,fname,style,dtcp):
    tt = time.time(); startt = tt
    # create a cursor, though most of the time turns out we don't need it because the connection
    # also has an execute() method.
    cur = cnx.cursor()
    # declare some custom functions to use within SQL queries (awesome!)
    cnx.create_function("grs",2,ifgrp)
    cnx.create_function("shw",2,shortenwords)
    cnx.create_function("drl",1,dropletters)
    cnx.create_aggregate("dgr",2,diaggregate)
    cnx.create_aggregate("igr",11,infoaggregate)
    cnx.create_aggregate("xgr",11,debugaggregate)
    # not quite foolproof-- still pulls in PROCID's, but in the final version we'll be filtering on this
    icd9grep = '.*\\\\([VE0-9]{3}(\\.[0-9]{0,2}){0,1})\\\\.*'
    loincgrep = '\\\\([0-9]{4,5}-[0-9])\\\\COMPONENT'

    ag = cnx.execute("select concept_cd,modifier_cd,instance_num,valtype_cd,tval_char,nval_num,valueflag_cd,quantity_num,units_cd,location_cd,confidence_num from observation_fact").fetchall()[0:50]
    da = debugaggregate()
    import pdb; pdb.set_trace()    
    # todo: make these passable via command-line argument for customizability
    binvals = ['No','Yes']
    # DONE (ticket #1): instead of relying on sqlite_denorm.sql, create the scaffold table from inside this 
    # script by putting the appropriate SQL commands into character strings and then passing those
    # strings as arguments to execute() (see below for an example of cur.execute() usage (cur just happens 
    # to be what we named the cursor object we created above, and execute() is a method that cursor objects have)
    # DONE: create an id to concept_cd mapping table (and filtering out redundant facts taken care of here)
    # TODO: parameterize the fact-filtering
    # create a log table
    cnx.execute("""create table if not exists dflog as
      select datetime() timestamp,
      'FirstEntryKey                                     ' key,
      'FirstEntryVal                                     ' val""")
    
    # certain values should not be changed after the first run
    cnx.execute("CREATE TABLE if not exists dfvars ( varname TEXT, textval TEXT, numval NUM )")
    olddtcp = cnx.execute("select numval from dfvars where varname = 'dtcp'").fetchall()
    if len(olddtcp) == 0:
      cnx.execute("insert into dfvars (varname,numval) values ('dtcp',"+str(dtcp)+")")
      cnx.commit()
      print "First run since cleanup, apparently"
    elif len(olddtcp) == 1:
      if dtcp != olddtcp:
	dtcp = olddtcp[0][0]
	print "Warning! Ignoring requested datecompress value and using previously stored value of "+str(dtcp)
	print "To get rid of it, do `python makesql.py -c dbfile`"
    else:
      print "Uh oh. Something is wrong there should not be more than one 'dtcp' entry in dfvars, debug time"
      import pdb; pdb.set_trace()    
        
    if cnx.execute("select count(*) from modifier_dimension").fetchone()[0] == 0:
      print "modifier_dimension is empty, let's fill it"
      # we load our local fallback db
      cnx.execute("attach './sql/datafinisher.db' as dfdb")
      # and copy from it into the input .db file's modifier_dimension
      cnx.execute("insert into modifier_dimension select * from dfdb.modifier_dimension")
      # and log that we did so
      cnx.execute("insert into dflog select datetime(),'insert','modifier_dimension'")
      cnx.commit()
    tprint("initialized variables",tt);tt = time.time()

    # cur.execute("drop table if exists scaffold")
    # turns out it was not necessary to create an empty table first for scaffold-- the date problem 
    # that this was supposed to solve was being caused by something else, so here is the more concise
    # version that may also be a little faster
    cnx.execute("""create table if not exists scaffold as
    select distinct patient_num, """+rdst(dtcp)+""" start_date
    from observation_fact order by patient_num, start_date;
    """)
    cnx.execute("CREATE UNIQUE INDEX if not exists df_ix_scaffold ON scaffold (patient_num,start_date) ")
    tprint("created scaffold table and index",tt);tt = time.time()

    # cnx.execute("drop table if exists cdid")
    cnx.execute(cdid_tab)
    tprint("created cdid table",tt);tt = time.time()

    # diagnoses
    cnx.execute("""update cdid set cpath = grs('"""+icd9grep+"""',cpath) where ddomain like '%|DX_ID' """)
    cnx.execute("""update cdid set cpath = substr(ccd,instr(ccd,':')+1) where ddomain = 'ICD9'""")
    # LOINC
    cnx.execute("""update cdid set cpath = grs('"""+loincgrep+"""',cpath) where ddomain like '%|COMPONENT_ID' """)
    cnx.execute("""update cdid set cpath = substr(ccd,instr(ccd,':')+1) where ddomain = 'LOINC'""")
    cnx.execute("create UNIQUE INDEX if not exists df_ix_cdid ON cdid (id,cpath,ccd)")
    cnx.commit()
    tprint("mapped concept codes in cdid",tt);tt = time.time()

    # create a couple of cleaned-up views of observation_fact
    # replace most of the non-informative values with nulls, remove certain known redundant modifiers
    cur.execute("drop view if exists obs_all")
    cur.execute(obs_all_v);
    cur.execute("drop view if exists obs_noins")
    # it would be better to aggregate multiple numeric values of the same fact collected on the same day by median, but alas
    # not all versions of SQLite have support for the median function
    cur.execute(obs_noins_v);
    tprint("created obs_all and obs_noins views",tt);tt = time.time()
    
    cnx.execute("drop view if exists obs_codemod")
    cnx.execute("create view obs_codemod as select distinct patient_num pn,"+rdst(dtcp)+""" sd,id,concept_cd
		,replace("""+dfctcode(mod="case when modifier_cd in ('','@') then null else modifier_cd end")+""",'mod',concept_cd) modifier_cd
		from observation_fact join cdid on concept_cd = ccd
		where modifier_cd not in ('Labs|Aggregate:Median','Labs|Aggregate:Last'
		  ,'DiagObs:MEDICAL_HX','PROBLEM_STATUS_C:2','PROBLEM_STATUS_C:3','DiagObs:PROBLEM_LIST'
		  ,'PROCORDERS:Outpatient')
		group by patient_num,"""+rdst(dtcp)+""",concept_cd,id
		""")
    
    cur.execute("drop view if exists obs_diag_active")
    cur.execute(obs_diag_inactive_v)
    #       ,replace("""+dfctcode(mod='modifier_cd').replace("'mod","||cpath'")+""",'DiagObs:','') modifier_cd 
    # ,replace(replace("""+dfctcode(mod='modifier_cd')+""",'DiagObs:',''),'mod',cpath) modifier_cd 
    tprint("created obs_diag_active view",tt);tt = time.time()

    cur.execute("drop view if exists obs_diag_inactive")
    cur.execute(obs_diag_inactive_v)
    tprint("created obs_diag_inactive view",tt);tt = time.time()

    cur.execute("drop view if exists obs_labs")
    cur.execute(obs_labs_v)
    tprint("created obs_labs view",tt);tt = time.time()
    
    # DONE: instead of a with-clause temp-table create a static data dictionary table
    #		var(concept_path,concept_cd,ddomain,vid) 
    # BTW, turns out this is a way to read and execute a SQL script
    # TODO: the shortened column names will go into this data dictionary table
    # DONE: create a filtered static copy of OBSERVATION_FACT with a vid column, maybe others
    # no vid column, relationship between concept_cd and id is not 1:1, so could get too big
    # will instead cross-walk the cdid table as needed
    # ...but perhaps unnecessary now that cdid table exists
    
    #cur.execute("drop table if exists data_dictionary")
    with open(ddsql,'r') as ddf:
	ddcreate = ddf.read()
    cur.execute(ddcreate)
    # rather than running the same complicated select statement multiple times for each rule in data_dictionary
    # lets just run each selection criterion once and save it as a tag in the new RULE column
    tprint("created data_dictionary",tt);tt = time.time()

    # diagnosis
    cur.execute(dd_diag)
    # LOINC
    cur.execute(dd_loinc)
    # code-only
    cur.execute(dd_codeonly)
    # code-and-mod only
    cur.execute(dd_codemod_only)
    # of the concepts in this column, only one is recorded at a time
    cur.execute(dd_oneperday)
    cnx.commit()
    tprint("added rules to data_dictionary",tt);tt = time.time()
    
    cur.execute(codef_sel)
    codesel = cur.fetchone()[0]
    # dynamically generate the terms in the select statement
    # extract the terms that meet the above criterion
    codeqry = "create table if not exists codefacts as select scaffold.*,"+codesel+" from scaffold "
    # now dynamically generate the many, many join clauses and append them to codefacts
    # note the string replace-- cannot alias the table name in an update statement, so no dd
    cur.execute(codef_jrep)
    codeqry += " ".join([row[0] for row in cur.fetchall()])
    tprint("created dynamic SQL for codefacts",tt);tt = time.time()

    cur.execute(codeqry) 
    tprint("created codefacts table",tt);tt = time.time()
    # same pattern as above, but now for facts that consist of both codes and modifiers

    # select terms...
    cur.execute(codemod_sel)
    codemodsel = cur.fetchone()[0]
    codemodqry = "create table if not exists codemodfacts as select scaffold.*,"+codemodsel+" from scaffold "
    # ...and joins...
    cur.execute(codemod_jrep)
    #import pdb; pdb.set_trace()
    codemodqry += " ".join([row[0] for row in cnx.execute(codemodx).fetchall()])
    tprint("created dynamic SQL for codemodfacts",tt);tt = time.time()

    cur.execute(codemodqry)
    tprint("created codemodfacts table",tt);tt = time.time()
    
    # DONE: cid's (column id's i.e. groups of variables that were selected together by the researcher)
    # ...cid's that have a ccd value of 1 (meaning there is only one distinct concept code per cid
    # any variable that doesn't have multiple values on the same day 
    # (except multiple instances of numeric values which get averaged)
    # these are expected to be numeric variables
    # TODO: create a column in obs_noins with a count of duplicates that got averaged, for QC
    # here are the select terms, but a little more complicated than in the above cases
    # on the fence whether to have extra column for the code
    # ','||colid||'_cd'||
    cur.execute(opd_sel)
    oneperdaysel = " ".join([row[0] for row in cur.fetchall()])
    oneperdayqry = "create table if not exists oneperdayfacts as select scaffold.*" + oneperdaysel + " from scaffold "
    # since we're doing ALL the non-aggregate columns at the same time, the above query is designed
    # to produce multiple rows, so we change the earlier pattern slightly so we can glue them all together
    # joins
    cur.execute(opd_jrep)
    oneperdayqry += " ".join([row[0] for row in cur.fetchall()])
    tprint("created dynamic SQL for oneperday",tt);tt = time.time()

    cur.execute(oneperdayqry)
    tprint("created oneperdayfacts table",tt);tt = time.time()
    # diagnoses output tables

    cur.execute(diag_sel)
    diagsel = cur.fetchone()[0]
    diagqry = "create table if not exists diagfacts as select scaffold.*,"+diagsel+" from scaffold "
    cur.execute(diag_jrep)
      ,group_concat(distinct modifier_cd) '||colid||' from obs_diag_active where id='||cid||' group by pn,sd) '||colid||' on '||colid||'.pn = scaffold.patient_num and '||colid||'.sd = scaffold.start_date' from data_dictionary where rule ='diag'
    diagqry += " ".join([row[0] for row in cur.fetchall()])
    tprint("created dynamic SQL for diag",tt);tt = time.time()
    cur.execute(diagqry)
    tprint("created diagfacts table",tt);tt = time.time()
    
    # DONE: create the LOINCFACTS table which will contain: pn,sd,nval_num,units,info,and cpath as part of the colid
    loincsel = cnx.execute(loinc_sel).fetchone()[0]
    loincqry = "create table if not exists loincfacts as select scaffold.*,"+loincsel+" from scaffold "
    # okay, so the below is insane and should probably be refactored
    # We have the usual " ".join(blah blah blah) to create the join clauses
    # But the query that creates those clauses replaces all hyphens with underscores so that the
    # dynamically generated column names will be legal ones... but in one place in each subquery, 
    # there really should be a hyphen instead of an uderscore: where the cpath is matched to a 
    # LOINC code. So, on the python side, we change those and only those underscores back to hyphens
    # I know, pretty f*ck*d up, isn't it?
    loincqry += re.compile("cpath=(['][[0-9]{4,5})_").sub(r'cpath=\1-'," ".join([row[0] for row in cnx.execute(loinc_jrep).fetchall()]))
    tprint("created dynamic SQL for loinc",tt);tt = time.time()
    cnx.execute(loincqry)
    tprint("created loincfacts table",tt);tt = time.time()
   
    # DONE: fallback on giant messy concatenated strings for everything else (for now)
    cur.execute(utemp_sel)
    unkqryvars = cur.fetchone()
    if unkqryvars[2] != None:
      unkqry0 = "create table if not exists unktemp as select patient_num,start_date,id,"
      unkqry0 += dfctday(cd="concept_cd",md="modifier_cd",ix="instance_num",tp="valtype_cd",\
	tv="tval_char",nv="nval_num",fl="valueflag_cd",qt="quantity_num",un="units_cd",\
	  lc="location_cd",cf="confidence_num") + " megacode from "
      unkqry0 += """obs_all join cdid on concept_cd = ccd
		    where id in ("""+unkqryvars[2]+") group by patient_num,start_date,id"

      """
	  ,group_concat(distinct concept_cd||coalesce('&mod='||modifier_cd,'')||
	  coalesce('&ins='||instance_num,'')||coalesce('&typ='||valtype_cd,'')||
	  coalesce('&txt='||tval_char,'')||coalesce('&num='||nval_num,'')||
	  coalesce('&flg='||valueflag_cd,'')||coalesce('&qty='||quantity_num,'')||
	  coalesce('&unt='||units_cd,'')||coalesce('&loc='||location_cd,'')||
	  coalesce('&cnf='||confidence_num,'')) megacode
      """
      unkqry1 = "create table if not exists unkfacts as select scaffold.*,"+unkqryvars[0]+" from scaffold "
      unkqry1 += unkqryvars[1]
      tprint("created dynamic SQL for unktemp and unkfacts tables",tt);tt = time.time()
      cur.execute(unkqry0)
      tprint("created unktemp table",tt);tt = time.time()
      cur.execute(unkqry1)
      tprint("created unkfacts table",tt);tt = time.time()

    # DONE: except we don't actually do it yet-- need to play with the variables and see the cleanest way to merge
    # the individual tables together
    # TODO: revise for consistent use of commas
    allsel = rdt('birth_date',dtcp)+""" birth_date, sex_cd 
      ,language_cd, race_cd, julianday(scaffold.start_date) - julianday("""+rdt('birth_date',dtcp)+") age_at_visit_days,"
    allsel += diagsel+','+loincsel+','+codesel+','+codemodsel+oneperdaysel
    if unkqryvars[0] != None:
      allsel += ','+unkqryvars[0]
    allqry = "create table if not exists fulloutput as select scaffold.*,"+allsel
    allqry += """ from scaffold 
    left join patient_dimension pd on pd.patient_num = scaffold.patient_num
    left join diagfacts df on df.patient_num = scaffold.patient_num and df.start_date = scaffold.start_date
    left join loincfacts lf on lf.patient_num = scaffold.patient_num and lf.start_date = scaffold.start_date
    left join codefacts cf on cf.patient_num = scaffold.patient_num and cf.start_date = scaffold.start_date 
    left join codemodfacts cmf on cmf.patient_num = scaffold.patient_num and cmf.start_date = scaffold.start_date 
    left join oneperdayfacts one on one.patient_num = scaffold.patient_num and one.start_date = scaffold.start_date """
    if unkqryvars[2] != None:
      allqry += "left join unkfacts unk on unk.patient_num = scaffold.patient_num and unk.start_date = scaffold.start_date "
    allqry += " order by patient_num, start_date"
    cur.execute(allqry)
    tprint("created fulloutput table",tt);tt = time.time()

    binoutqry = """create view binoutput as select patient_num,start_date,birth_date,sex_cd
		   ,language_cd,race_cd,age_at_visit_days"""
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(diagfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([ii[1] for ii in cnx.execute("pragma table_info(loincfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(codefacts)").fetchall()[2:]])
    binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			"' end "+ii[1] for ii in cnx.execute("pragma table_info(codemodfacts)").fetchall()[2:]])
    binoutqry += ","+",".join([ii[1] for ii in cnx.execute("pragma table_info(oneperdayfacts)").fetchall()[2:]])
    if unkqryvars[2] != None:
      binoutqry += ","+",".join([" case when "+ii[1]+" is null then '"+binvals[0]+"' else '"+binvals[1]+\
			  "' end "+ii[1] for ii in cnx.execute("pragma table_info(unkfacts)").fetchall()[2:]])
    binoutqry += " from fulloutput"
    cnx.execute("drop view if exists binoutput")
    cnx.execute(binoutqry)
    tprint("created binoutput view",tt);tt = time.time()

    if style == 'simple':
      finalview = 'binoutput'
    else:
      finalview = 'fulloutput'
      
    if fname.lower() != 'none':
      ff = open(fname,'wb')
      csv.writer(ff).writerow([ii[1] for ii in con.execute("PRAGMA table_info(fulloutput)").fetchall()])
      result = cnx.execute("select * from "+finalview).fetchall()
      with ff:
	  csv.writer(ff).writerows(result)
    # DONE: write 'select * from fulloutput' to the csvfile. Should it be passed to main as a parameter? (yes)
    # TODO: create a view that replaces the various strings with simple 1/0 values
    tprint("wrote output table to file",tt);tt = time.time()
    tprint("TOTAL RUNTIME",startt)
    import pdb; pdb.set_trace()    
        
    # Boom! We covered all the cases. Messy, but at least a start.

    # the below yeah, I guess, but there are two big and easier to implement cases to do first


    """
    The decision process
      branch node
	uses mods DONE
	  map modifiers; single column of semicolon-delimited code=mod pairs
	uses other columns?
	  UNKNOWN FALLBACK, single column DONE
	code-only DONE
	  single column of semicolon-delimited codes
      leaf node
	code only DONE
	  single 1/0 column (TODO)
	uses code and mods only DONE
	  map modifiers; single column of semicolon-delimited mods DONE-ish
	uses other columns?
	  any columns besides mods have more than one value per patient-date?
	    UNKNOWN FALLBACK, single column DONE-ish
	  otherwise
	    map modifiers; single column of semicolon-delimited mods named FOO_mod; for each additional BAR, one more column FOO_BAR DONE-ish
    
    TODO: implement a user-configurable 'rulebook' containing patterns for catching data that would otherwise fall 
    into UNKNOWN FALLBACK, and expressing in a parseable form what to do when each rule is triggered.
    DONE: The data dictionary will contain information about which built-in or user-configured rule applies for each cid
    We are probably looking at several different 'dcat' style tables, broken up by type of data
    DONE: We will iterate through the data dictionary, joining new columns to the result according to the applicable rule
    """
    
    
	
if __name__ == '__main__':
    con = sq.connect(args.dbfile)

    if args.csvfile == 'OUTFILE':
      csvfile = args.dbfile.replace(".db","")+".csv"
    else:
      csvfile = args.csvfile

    if args.datecompress == 'week':
      dtcp = 7
    elif args.datecompress == 'month':
      dtcp = 365.0/12
    else:
      dtcp = args.datecompress
      
    #import pdb; pdb.set_trace();
    #import code; code.interact(local=vars())
    if args.cleanup:
      cleanup(con)
    else:
      main(con,csvfile,args.style,dtcp)



